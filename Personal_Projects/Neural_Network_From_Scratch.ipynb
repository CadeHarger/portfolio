{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeq6LtZHoiEd54jqxJHUG2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CadeHarger/portfolio/blob/main/Personal_Projects/Neural_Network_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piBoKpbntLnZ",
        "cellView": "form"
      },
      "source": [
        "#@title Import Relevant Modules\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "#from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twKCLklX-2je",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73cff497-611c-40f1-a71b-9c998935fbdd"
      },
      "source": [
        "#@title Load the MNIST Dataset\n",
        "np.set_printoptions(linewidth = 200)\n",
        "(fdataSet, labelSet),(testDataSet, testLabelSet) = tf.keras.datasets.mnist.load_data()\n",
        "dataSet = np.zeros((len(fdataSet), 784), dtype = np.float32)\n",
        "for x in range(len(dataSet)):\n",
        "  dataSet[x] = fdataSet[x].flatten()\n",
        "dataSet /= 255.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b9y15KgPusl0",
        "outputId": "7c092518-e2a8-498d-d2f7-eac94709bce8"
      },
      "source": [
        "class Node:\n",
        "  # Represents a node in the network\n",
        "  def __init__(self, count):\n",
        "    self.bias = 0\n",
        "    self.activation = []\n",
        "    self.z = []\n",
        "    self.weights = np.zeros((count), dtype=np.float32)\n",
        "    self.wGradients = []\n",
        "    self.bGradients = []\n",
        "  def enableNP(self, batchSize):\n",
        "    self.activation = np.zeros((batchSize), dtype = np.float32)\n",
        "    self.z = np.zeros((batchSize), dtype = np.float32)\n",
        "  def setZ(self, zeta, example):\n",
        "    self.z[example] = zeta\n",
        "  def setActivation(self, a, example):\n",
        "    self.activation[example] = a\n",
        "  def setWeights(self, w):\n",
        "    self.weights = w\n",
        "  def addWGradient(self, gradient):\n",
        "    self.wGradients.append(gradient)\n",
        "  def addBGradient(self, gradient):\n",
        "    self.bGradients.append(gradient)\n",
        "  def update(self, lr):\n",
        "    self.weights = np.subtract(self.weights, np.sum(self.wGradients, 0) * lr / len(self.wGradients))\n",
        "    self.bias -= np.sum(self.bGradients) * lr / len(self.bGradients)\n",
        "    wGradients = []\n",
        "    bGradients = []\n",
        "class InputNode:\n",
        "  # Represents an input node in the input layer\n",
        "  def __init__(self):\n",
        "    self.activation = 0\n",
        "  def enableNP(self, batchSize):\n",
        "    self.activation = np.zeros((batchSize), dtype = np.float32)\n",
        "  def setInput(self, n, example):\n",
        "    self.activation[example] = n\n",
        "  def setWeights(self, w):\n",
        "    pass\n",
        "  def update(self, lr):\n",
        "    pass\n",
        "\n",
        "\n",
        "class Model:\n",
        "  # Represents the neural network model\n",
        "  def __init__(self):\n",
        "    self.lossFunc = SquaredError()\n",
        "    self.layers = [] # Contains every layer in the network as lists of nodes\n",
        "    self.activations = [] # Contains the activation for every layer (with the same indexes)\n",
        "    self.values = [] # The key for one-hot encoded labels\n",
        "  def __init__(self, lf):\n",
        "    self.layers = []\n",
        "    self.activations = []\n",
        "    self.values = []\n",
        "    if lf == 'squaredError':\n",
        "      self.lossFunc = SquaredError()\n",
        "    elif lf == 'categoricalCrossEntropy':\n",
        "      self.lossFunc = CategoricalCrossEntropy()\n",
        "    else:\n",
        "      self.lossFunc = SquaredError()\n",
        "\n",
        "  def addLayer(self, units, activation):\n",
        "    # Adds a layer to the model\n",
        "    layer = [Node(len(self.layers[-1])) for x in range(units)] #Create every neuron and weight\n",
        "    self.layers.append(layer)\n",
        "    if activation == 'relu':\n",
        "      self.activations.append(Relu())\n",
        "    elif activation == 'softmax':\n",
        "      self.activations.append(Softmax())\n",
        "    else:\n",
        "      self.activations.append(ActivationFunction())\n",
        "  def addInput(self, size):\n",
        "    # Adds an input layer to the model\n",
        "    self.layers.append([InputNode() for x in range(size)])\n",
        "    self.activations.append(None)\n",
        "\n",
        "  def randomizeWeights(self, length, batchSize):\n",
        "    # Initializes all of the weights in the network to be random\n",
        "    for layer in range(len(self.layers)):\n",
        "      for node in self.layers[layer]:\n",
        "        if layer != 0:\n",
        "          prevLayerLen = len(self.layers[layer - 1])\n",
        "          node.setWeights((-1 / math.sqrt(prevLayerLen)) + np.random.rand(prevLayerLen) * (2 / math.sqrt(prevLayerLen))) #Xavier initialization?\n",
        "          #node.setWeights((1 / prevLayerLen) + np.random.rand(prevLayerLen) * (2 / prevLayerLen))\n",
        "        node.enableNP(batchSize)\n",
        "\n",
        "  def oneHotEncode(self, labelSet):\n",
        "    # One-hot encodes the label set and creates a key for the encodings\n",
        "    for value in labelSet:\n",
        "      if not value in self.values:\n",
        "        self.values.append(value)\n",
        "    temp = np.zeros((labelSet.shape[0], len(self.values)), dtype=np.int8)\n",
        "    for x in range(labelSet.shape[0]):\n",
        "      onehot = np.zeros((len(self.values)), dtype=np.int8)\n",
        "      onehot[self.values.index(labelSet[x])] = 1\n",
        "      temp[x] = onehot\n",
        "    return temp\n",
        "\n",
        "  def predict(self, data):\n",
        "    # Predicts outputs given a batch (or less, but not more) of input data\n",
        "    predictions = np.zeros((data.shape[0], len(self.layers[-1])), dtype=np.float32) #2d array storing every prediction\n",
        "    for example in range(len(data)): #Check the sum of each layers activations to check random initialization?\n",
        "      for x in range(data[example].shape[0]):\n",
        "        self.layers[0][x].setInput(data[example][x], example) #test whether list comprehension is faster?\n",
        "      predictions[example] = self.calculate(self.layers, self.activations, example)\n",
        "    return predictions\n",
        "  def calculate(self, resultLayer, layerActivation, example):\n",
        "    # Forward propogation by recursive matrix multiplication\n",
        "    if len(resultLayer) == 1:\n",
        "      inputs = np.zeros(len(resultLayer[0]), dtype=np.float32)\n",
        "      for x in range(len(resultLayer[0])):\n",
        "        inputs[x] = resultLayer[0][x].activation[example]\n",
        "      return inputs\n",
        "    else:\n",
        "      prevLayerOutput = self.calculate(resultLayer[:-1], layerActivation[:-1], example) #retrieve the activations of the previous layer\n",
        "      weitrix = np.zeros((len(resultLayer[-1]), prevLayerOutput.shape[0]), dtype=np.float32) # Each row is a node's weights\n",
        "      biases = np.zeros((len(resultLayer[-1])), dtype=np.float32) #test whether list comprehension is faster?\n",
        "      for node in range(len(resultLayer[-1])):\n",
        "        weitrix[node] = resultLayer[-1][node].weights\n",
        "        biases[node] = resultLayer[-1][node].bias\n",
        "      result = np.add(np.matmul(weitrix, prevLayerOutput), biases) # z\n",
        "      for x in range(result.shape[0]):\n",
        "        resultLayer[-1][x].setZ(result[x], example)\n",
        "        result[x] = layerActivation[-1].fx(result, x) # Applies the activation function\n",
        "        resultLayer[-1][x].setActivation(result[x], example)\n",
        "      return result\n",
        "\n",
        "  def train(self, epochs, dataSet, labelSet, batchSize, lr):\n",
        "    # Trains the model\n",
        "    if len(labelSet.shape) == 1 and len(self.layers[-1]) != 1:\n",
        "      labelSet = self.oneHotEncode(labelSet) # One-hot encode the labels if it needs it (detection flawed)\n",
        "    self.randomizeWeights(len(self.layers[0]), batchSize)\n",
        "    batchCount = int(dataSet.shape[0] / batchSize)\n",
        "    x_batches = np.zeros((batchCount, batchSize, dataSet.shape[1]), dtype=np.float32)\n",
        "    y_batches = np.zeros((batchCount, batchSize, labelSet.shape[1]), dtype=np.float32)\n",
        "    for x in range(batchCount):\n",
        "      x_batches[x] = dataSet[batchSize * x: batchSize * x + batchSize]\n",
        "      y_batches[x] = labelSet[batchSize * x: batchSize * x + batchSize]\n",
        "    for epoch in range(epochs):\n",
        "      print(\"Epoch: \", epoch)\n",
        "      for batch in range(x_batches.shape[0]):\n",
        "        print(\"   Batch: \", batch)\n",
        "        predictions = self.predict(x_batches[batch])\n",
        "        for example in range(predictions.shape[0]):\n",
        "          #BACKPROPOGATION (does ignoring input layer cause issues?)\n",
        "          #Do activations for previous layers get fully updated?\n",
        "          #Random Initialization only works for datasets normalized 0.0 to 1.0\n",
        "          #PrevLayerActivations causing ram issues?\n",
        "          #Test out creating a list for the partial derivatives and comparing speeds\n",
        "          #Research Jacobian Matrix\n",
        "          #Test all of the list comprehension comments\n",
        "          #Print loss after each batch?\n",
        "          #Double Check Math (Confidence that its learning is absolute but accuracy not high enough (improper calculation?))\n",
        "          #Time everything\n",
        "          activationGradients = []\n",
        "          for currentLayer in range(len(self.layers) - 1, 0, -1):\n",
        "            isOutput, prevLayerLen, zVectors = (currentLayer == len(self.layers) - 1), len(self.layers[currentLayer - 1]), np.zeros((len(self.layers[currentLayer])), dtype=np.float32)\n",
        "            activationGradients = np.zeros((prevLayerLen), dtype = np.float32)\n",
        "            for nodeNo in range(len(self.layers[currentLayer])):\n",
        "              zVectors[nodeNo] = self.layers[currentLayer][nodeNo].z[example] # List Comprehension? needs reconversion to numpy\n",
        "            if isOutput:\n",
        "              prevLayerActivations = self.lossFunc.dfx(predictions[example], y_batches[batch][example])\n",
        "            else:\n",
        "              prevLayerActivations = activationGradients\n",
        "            for nodeNo in range(len(self.layers[currentLayer])):\n",
        "              bGradient = self.activations[currentLayer].dfx(zVectors, nodeNo) * prevLayerActivations[nodeNo]\n",
        "              self.layers[currentLayer][nodeNo].addWGradient(np.array([self.layers[currentLayer - 1][x].activation[example] for x in range(prevLayerLen)]) * bGradient)\n",
        "              self.layers[currentLayer][nodeNo].addBGradient(bGradient)\n",
        "              activationGradients = np.add(activationGradients, np.array([self.layers[currentLayer][nodeNo].weights[x] for x in range(prevLayerLen)]) * bGradient)\n",
        "        #map(lambda y:map(lambda x:x.update(lr), y), self.layers) # Apply the computed gradients to every node\n",
        "        for layer in self.layers:\n",
        "          for node in layer:\n",
        "            node.update(lr)\n",
        "\n",
        "\n",
        "class ActivationFunction:\n",
        "  def __init__(self):\n",
        "    return\n",
        "  def fx(self, z, i):\n",
        "    return z[i]\n",
        "  def dfx(self, z):\n",
        "    return 1\n",
        "\n",
        "class Relu(ActivationFunction):\n",
        "  def fx(self, z, i):\n",
        "    return max(0, z[i])\n",
        "  def dfx(self, z, i):\n",
        "    return 1 * (z[i] > 0)\n",
        "\n",
        "class Softmax(ActivationFunction):\n",
        "  def fx(self, z, i):\n",
        "    return math.exp(z[i]) / np.sum(np.exp(z))\n",
        "  def dfx(self, z, i):\n",
        "    pi = self.fx(z, i)\n",
        "    return np.sum([pi * (1 - pi) if i == j else -1 * pi * self.fx(z, j) for j in range(z.shape[0])])\n",
        "\n",
        "\n",
        "class LossFunction:\n",
        "  def __init__(self):\n",
        "    return\n",
        "  def fx(self, a, y):\n",
        "    return y\n",
        "  def dfx(self, a, y):\n",
        "    return y\n",
        "\n",
        "class SquaredError(LossFunction):\n",
        "  def fx(self, a, y):\n",
        "    return (a - y) * (a - y)\n",
        "  def dfx(self, a, y):\n",
        "    return 2 * (a - y)\n",
        "\n",
        "class CategoricalCrossEntropy(LossFunction):\n",
        "  def fx(self, a, y):\n",
        "    return (-1 * y * np.log(np.exp(a) / np.sum(np.exp(a)))).min()\n",
        "  def dfx(self, a, y):\n",
        "    exps = np.exp(a)\n",
        "    return exps / np.sum(exps) - y\n",
        "\n",
        "model = Model(lf = 'categoricalCrossEntropy')\n",
        "model.addInput(784)\n",
        "model.addLayer(100, 'relu')\n",
        "model.addLayer(50, 'relu')\n",
        "model.addLayer(10, 'softmax')\n",
        "model.train(epochs = 1, dataSet = dataSet, labelSet = labelSet, batchSize = 400, lr = 0.003)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0\n",
            "   Batch:  0\n",
            "   Batch:  1\n",
            "   Batch:  2\n",
            "   Batch:  3\n",
            "   Batch:  4\n",
            "   Batch:  5\n",
            "   Batch:  6\n",
            "   Batch:  7\n",
            "   Batch:  8\n",
            "   Batch:  9\n",
            "   Batch:  10\n",
            "   Batch:  11\n",
            "   Batch:  12\n",
            "   Batch:  13\n",
            "   Batch:  14\n",
            "   Batch:  15\n",
            "   Batch:  16\n",
            "   Batch:  17\n",
            "   Batch:  18\n",
            "   Batch:  19\n",
            "   Batch:  20\n",
            "   Batch:  21\n",
            "   Batch:  22\n",
            "   Batch:  23\n",
            "   Batch:  24\n",
            "   Batch:  25\n",
            "   Batch:  26\n",
            "   Batch:  27\n",
            "   Batch:  28\n",
            "   Batch:  29\n",
            "   Batch:  30\n",
            "   Batch:  31\n",
            "   Batch:  32\n",
            "   Batch:  33\n",
            "   Batch:  34\n",
            "   Batch:  35\n",
            "   Batch:  36\n",
            "   Batch:  37\n",
            "   Batch:  38\n",
            "   Batch:  39\n",
            "   Batch:  40\n",
            "   Batch:  41\n",
            "   Batch:  42\n",
            "   Batch:  43\n",
            "   Batch:  44\n",
            "   Batch:  45\n",
            "   Batch:  46\n",
            "   Batch:  47\n",
            "   Batch:  48\n",
            "   Batch:  49\n",
            "   Batch:  50\n",
            "   Batch:  51\n",
            "   Batch:  52\n",
            "   Batch:  53\n",
            "   Batch:  54\n",
            "   Batch:  55\n",
            "   Batch:  56\n",
            "   Batch:  57\n",
            "   Batch:  58\n",
            "   Batch:  59\n",
            "   Batch:  60\n",
            "   Batch:  61\n",
            "   Batch:  62\n",
            "   Batch:  63\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c3876daa4e12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-c3876daa4e12>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, dataSet, labelSet, batchSize, lr)\u001b[0m\n\u001b[1;32m    165\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnodeNo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddWGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevLayerLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnodeNo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddBGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m               \u001b[0mactivationGradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivationGradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnodeNo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevLayerLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;31m#map(lambda y:map(lambda x:x.update(lr), y), self.layers) # Apply the computed gradients to every node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c3876daa4e12>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnodeNo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddWGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevLayerLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnodeNo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddBGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m               \u001b[0mactivationGradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivationGradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrentLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnodeNo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevLayerLen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;31m#map(lambda y:map(lambda x:x.update(lr), y), self.layers) # Apply the computed gradients to every node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIHJoRp4hM54",
        "outputId": "689599d7-a5c8-4150-a663-c3f8e278c0e7"
      },
      "source": [
        "test = model.predict(dataSet[4000:4020])\n",
        "print(model.values)\n",
        "print(test)\n",
        "print(labelSet[4000:4020])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 0, 4, 1, 9, 2, 3, 6, 7, 8]\n",
            "[[0.0973471  0.10171684 0.09842508 0.09840237 0.09686217 0.09354674 0.09587026 0.08968022 0.08964325 0.09280908]\n",
            " [0.09813985 0.10389046 0.09434298 0.09765258 0.10133725 0.09671469 0.08936594 0.09208387 0.09349604 0.08929063]\n",
            " [0.09683602 0.10607359 0.09458646 0.10036898 0.09826672 0.09272058 0.09127555 0.09059379 0.08894579 0.09260476]\n",
            " [0.09701217 0.10001724 0.09569116 0.09724838 0.10013254 0.09415543 0.0933549  0.09067732 0.0913     0.09227415]\n",
            " [0.09684304 0.10446884 0.09773659 0.09906532 0.10028784 0.09340028 0.08944467 0.093858   0.08965965 0.09233219]\n",
            " [0.09851065 0.10435416 0.09416638 0.09957847 0.09718162 0.0934089  0.09014022 0.09247489 0.08910544 0.09500491]\n",
            " [0.09896956 0.10154279 0.09565723 0.09924411 0.0987475  0.09400031 0.09117657 0.09273111 0.09192222 0.09203524]\n",
            " [0.09851333 0.10559402 0.09418648 0.0993282  0.10051779 0.09387708 0.0908221  0.09416704 0.09117511 0.09050097]\n",
            " [0.09915796 0.10419482 0.09653349 0.09846865 0.10043537 0.09187201 0.08868554 0.09387674 0.0901746  0.09224992]\n",
            " [0.09718244 0.10553414 0.0955896  0.10019216 0.09858847 0.09358371 0.08732969 0.09475514 0.08937651 0.09127934]\n",
            " [0.09877767 0.10068683 0.09537524 0.09742136 0.09935042 0.09372133 0.09374912 0.09173173 0.09125175 0.09324821]\n",
            " [0.09767168 0.10569227 0.09406412 0.09784652 0.10231842 0.09553201 0.08604121 0.09526441 0.09143437 0.08987698]\n",
            " [0.09877829 0.10133736 0.09708888 0.09896053 0.09932126 0.09305074 0.0923752  0.09227449 0.09049214 0.09226314]\n",
            " [0.09908706 0.10167492 0.09617963 0.09758577 0.09961434 0.09403688 0.09224553 0.09301288 0.09059637 0.09219475]\n",
            " [0.09903023 0.10211994 0.0953979  0.10032389 0.09973545 0.09328198 0.09143276 0.09199191 0.09031165 0.09266624]\n",
            " [0.09895837 0.10580292 0.09369039 0.09593959 0.10207818 0.09207521 0.09197304 0.0909113  0.09137743 0.09119871]\n",
            " [0.09654111 0.10444802 0.09411523 0.09517051 0.10268289 0.09376814 0.09347417 0.09110988 0.0915379  0.09010201]\n",
            " [0.09869742 0.10466317 0.09414122 0.09784008 0.09533986 0.09426633 0.09085162 0.09178302 0.09004144 0.0936622 ]\n",
            " [0.09883406 0.10473133 0.0969996  0.09940359 0.09747966 0.0928221  0.09156123 0.0921036  0.091772   0.09059739]\n",
            " [0.10052092 0.10276613 0.09548678 0.09830949 0.09876939 0.09318599 0.09142534 0.0922704  0.09125078 0.09311979]]\n",
            "[7 3 0 1 5 2 8 5 0 0 1 3 1 1 1 5 2 5 6 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSOXHo30bqRi",
        "outputId": "d5acc26d-9c59-4472-9b46-2ed845b714b0"
      },
      "source": [
        "def accuracy(predictions, labels, needsOneHot):\n",
        "  #Look in book for ml definition\n",
        "  #Create user friendly predict function (batch size independent, no backprop prep)\n",
        "  newLabels = labels.copy\n",
        "  total = 0.0\n",
        "  for x in range(len(predictions)):\n",
        "    if needsOneHot:\n",
        "      total += 1 * np.where(predictions[x] == np.max(predictions[x]))[0][0] == labels[x]\n",
        "\n",
        "  return total / len(predictions)\n",
        "\n",
        "total = 0.0\n",
        "bat = 400\n",
        "full = 20000\n",
        "for x in range(0, full, bat):\n",
        "  print(x)\n",
        "  total += accuracy(model.predict(dataSet[x:x + bat]), labelSet[x:x + bat], True)\n",
        "print(total / (full / bat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "400\n",
            "800\n",
            "1200\n",
            "1600\n",
            "2000\n",
            "2400\n",
            "2800\n",
            "3200\n",
            "3600\n",
            "4000\n",
            "4400\n",
            "4800\n",
            "5200\n",
            "5600\n",
            "6000\n",
            "6400\n",
            "6800\n",
            "7200\n",
            "7600\n",
            "8000\n",
            "8400\n",
            "8800\n",
            "9200\n",
            "9600\n",
            "10000\n",
            "10400\n",
            "10800\n",
            "11200\n",
            "11600\n",
            "12000\n",
            "12400\n",
            "12800\n",
            "13200\n",
            "13600\n",
            "14000\n",
            "14400\n",
            "14800\n",
            "15200\n",
            "15600\n",
            "16000\n",
            "16400\n",
            "16800\n",
            "17200\n",
            "17600\n",
            "18000\n",
            "18400\n",
            "18800\n",
            "19200\n",
            "19600\n",
            "0.12774999999999997\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}